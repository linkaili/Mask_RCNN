{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN for DeepScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our own dataset DeepScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnl\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as lines\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "from mrcnn.config import Config\n",
    "\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Import Mask RCNN\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib, utils\n",
    "\n",
    "# for mask\n",
    "import pathlib\n",
    "from skimage.io import imread, imsave, imshow\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patchess\n",
    "\n",
    "import skimage\n",
    "\n",
    "# process xml file\n",
    "import xml.etree.ElementTree\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "# through the command line argument --logs\n",
    "DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet50\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        512\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_MAX_DIM                  256\n",
      "IMAGE_META_SIZE                127\n",
      "IMAGE_MIN_DIM                  256\n",
      "IMAGE_MIN_SCALE                2.0\n",
      "IMAGE_RESIZE_MODE              crop\n",
      "IMAGE_SHAPE                    [256 256   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               512\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (28, 28)\n",
      "NAME                           symbols\n",
      "NUM_CLASSES                    115\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        2000\n",
      "POST_NMS_ROIS_TRAINING         1000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000.0\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           512\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50.0\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ScoreConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"symbols\"\n",
    "\n",
    "    # Backbone network architecture\n",
    "    # Supported values are: resnet50, resnet101\n",
    "    BACKBONE = \"resnet50\"\n",
    "    \n",
    "    # Input image resizing\n",
    "    # Random crops of size 512x512\n",
    "    IMAGE_RESIZE_MODE = \"crop\"\n",
    "    IMAGE_MIN_DIM = 256\n",
    "    IMAGE_MAX_DIM = 256\n",
    "    IMAGE_MIN_SCALE = 2.0\n",
    "    \n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    \n",
    "    # If enabled, resizes instance masks to a smaller size to reduce\n",
    "    # memory load. Recommended when using high-resolution images.\n",
    "    USE_MINI_MASK = True\n",
    "    MINI_MASK_SHAPE = (28, 28)  # (height, width) of the mini-mask\n",
    "    \n",
    "    # ROIs kept after non-maximum supression (training and inference)\n",
    "    POST_NMS_ROIS_TRAINING = 1000\n",
    "    POST_NMS_ROIS_INFERENCE = 2000\n",
    "    \n",
    "    # Number of training and validation steps per epoch\n",
    "    STEPS_PER_EPOCH = 1000/IMAGES_PER_GPU\n",
    "    VALIDATION_STEPS = 50/IMAGES_PER_GPU\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 114  # background + 114 symbols\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "    \n",
    "    # Number of ROIs per image to feed to classifier/mask heads\n",
    "    # The Mask RCNN paper uses 512 but often the RPN doesn't generate\n",
    "    # enough positive proposals to fill this and keep a positive:negative\n",
    "    # ratio of 1:3. You can increase the number of proposals by adjusting\n",
    "    # the RPN NMS threshold.\n",
    "    TRAIN_ROIS_PER_IMAGE = 512\n",
    "\n",
    "    # Maximum number of ground truth instances to use in one image\n",
    "    MAX_GT_INSTANCES = 512\n",
    "\n",
    "    # Max number of final detections per image\n",
    "    DETECTION_MAX_INSTANCES = 512\n",
    "\n",
    "    \n",
    "config = ScoreConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_score(self, dataset_dir, subset, split):\n",
    "        \"\"\"Load a subset of the DeepScore dataset.\n",
    "        dataset_dir: Root directory of the dataset.\n",
    "        subset: Subset to load: train or val\n",
    "        \"\"\"\n",
    "    \n",
    "        for key, value in class_dict.items():\n",
    "            self.add_class(\"symbol\", value, key)\n",
    "        # Train or validation dataset? \n",
    "        assert subset in [\"train\", \"val\"]\n",
    "        img_dir = pathlib.Path(dataset_dir).glob('*/images_png_small/*.png')       \n",
    "        img_sorted = sorted([x for x in img_dir])\n",
    "        xml_dir = pathlib.Path(dataset_dir).glob('*/xml_annotations_small/*.xml')       \n",
    "        xml_sorted = sorted([x for x in xml_dir])\n",
    "        mask_dir = pathlib.Path(dataset_dir).glob('*/pix_annotations_png_small/*.png')\n",
    "        mask_sorted = sorted([x for x in mask_dir])\n",
    "        if subset == \"train\":\n",
    "            img_sorted = img_sorted[:split]\n",
    "            xml_sorted = xml_sorted[:split]\n",
    "            mask_sorted = mask_sorted[:split]\n",
    "        if subset == \"val\":\n",
    "            img_sorted = img_sorted[split:]\n",
    "            xml_sorted = xml_sorted[split:]\n",
    "            mask_sorted = mask_sorted[split:]\n",
    "        \n",
    "        # add images\n",
    "        for i, image_path in enumerate(img_sorted):\n",
    "#             image = imread(str(image_path))\n",
    "#             height, width = image.shape[:2]\n",
    "            image_name = os.path.basename(image_path)\n",
    "            xml_path = xml_sorted[i]\n",
    "            symbols, _, height, width = get_symbol_info(xml_path)\n",
    "            \n",
    "            mask_path = str(mask_sorted[i])\n",
    "            # only select scores with less than 500 symbols\n",
    "#             if len(symbols) < 200:\n",
    "            self.add_image(\n",
    "                \"symbol\",\n",
    "                image_id=image_name,\n",
    "                path=image_path,\n",
    "                width=width, height=height,\n",
    "                symbols=symbols, mask_path=mask_path)\n",
    "\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the score data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"symbol\":\n",
    "            return info[\"path\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "    \n",
    "    def load_box(self, image_id):\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"symbol\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "        symbols = image_info['symbols']\n",
    "        boxes = np.zeros([len(symbols), 4], dtype=np.int32)\n",
    "        for i, symbol in enumerate(symbols):\n",
    "            # coords are row, col, so we should put (y, x), instead of (x, y)\n",
    "            xmin, xmax, ymin, ymax = symbol[1], symbol[2], symbol[3], symbol[4]\n",
    "            boxes[i] = np.array([ymin, xmin, ymax, xmax])\n",
    "        return boxes.astype(np.int32)\n",
    "    \n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for an image.\n",
    "        Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"        \n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"symbol\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "\n",
    "        # image_id == xml_id\n",
    "        symbols = image_info['symbols']\n",
    "        mask = imread(image_info['mask_path'])\n",
    "        masks = np.zeros([image_info['height'], image_info['width'], len(symbols)], dtype=np.uint8)\n",
    "        for i, symbol in enumerate(symbols):\n",
    "            # coords are row, col, so we should put (y, x), instead of (x, y)\n",
    "            xmin, xmax, ymin, ymax = symbol[1], symbol[2], symbol[3], symbol[4]\n",
    "            masks[ymin:ymax+1, xmin:xmax+1, i] = mask[ymin:ymax+1, xmin:xmax+1]\n",
    "        \n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in symbols])\n",
    "            \n",
    "        \n",
    "        return masks.astype(np.bool), class_ids.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    # Training dataset.\n",
    "    dataset_train = ScoreDataset()\n",
    "    dataset_train.load_score(dataset_dir, \"train\", split)\n",
    "    dataset_train.prepare()\n",
    "\n",
    "    # Validation dataset\n",
    "    dataset_val = ScoreDataset()\n",
    "    dataset_val.load_score(dataset_dir, \"val\", split)\n",
    "    dataset_val.prepare()\n",
    "\n",
    "    # *** This training schedule is an example. Update to your needs ***\n",
    "    # Since we're using a very small dataset, and starting from\n",
    "    # COCO trained weights, we don't need to train too long. Also,\n",
    "    # no need to train all layers, just the heads should do it.\n",
    "    print(\"Training network heads\")\n",
    "    model.train(dataset_train, dataset_val,\n",
    "                learning_rate=config.LEARNING_RATE,\n",
    "                epochs=30,\n",
    "                layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\Mask_RCNN\\images_png_small\\lg-46690-aug-gutenberg1939-.png\n",
      "2\n",
      "..\\..\\Mask_RCNN\\pix_annotations_png_small\\lg-46690-aug-gutenberg1939-.png\n",
      "..\\..\\Mask_RCNN\\xml_annotations_small\\lg-46690-aug-gutenberg1939-.xml\n"
     ]
    }
   ],
   "source": [
    "# Glob the training data and load a single image path\n",
    "img_paths = pathlib.Path('../../').glob('*/images_png_small/*.png')\n",
    "img_sorted = sorted([x for x in img_paths])\n",
    "\n",
    "# mask and xml files\n",
    "mask_paths = pathlib.Path('../../').glob('*/pix_annotations_png_small/*.png')\n",
    "mask_sorted = sorted([x for x in mask_paths])\n",
    "\n",
    "xml_paths = pathlib.Path('../../').glob('*/xml_annotations_small/*.xml')\n",
    "xml_sorted = sorted([x for x in xml_paths])\n",
    "\n",
    "# check the image, mask and xml path names are in the same order\n",
    "rand_img = 1\n",
    "im_path = img_sorted[rand_img]\n",
    "mask_path = mask_sorted[rand_img]\n",
    "xml_path = xml_sorted[rand_img]\n",
    "num_samples = len(img_sorted)\n",
    "print(im_path)\n",
    "print(len(img_sorted))\n",
    "print(mask_path)\n",
    "print(xml_path)\n",
    "im = imread(str(im_path))\n",
    "mask = imread(str(mask_path))\n",
    "root = xml.etree.ElementTree.parse(str(xml_path)).getroot()\n",
    "size = root.findall('size')\n",
    "width = float(size[0][0].text)\n",
    "height = float(size[0][1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the information of all symbols in one image\n",
    "def get_symbol_info(xml_path):\n",
    "    root = xml.etree.ElementTree.parse(str(xml_path)).getroot()\n",
    "    size = root.findall('size')\n",
    "    width = float(size[0][0].text)\n",
    "    height = float(size[0][1].text)\n",
    "\n",
    "    symbols = []\n",
    "    symbol_names = set() # use a set to store unique symbol names\n",
    "    rectangles = []\n",
    "\n",
    "    # get the bounding box for each object, multiply with its width and height to get the real pixel coords\n",
    "    for symbol in root.findall('object'):\n",
    "        name = symbol.find('name').text\n",
    "        xmin = round(float(symbol.find('bndbox')[0].text)*width)\n",
    "        xmax = round(float(symbol.find('bndbox')[1].text)*width)\n",
    "        ymin = round(float(symbol.find('bndbox')[2].text)*height)\n",
    "        ymax = round(float(symbol.find('bndbox')[3].text)*height)\n",
    "\n",
    "#         current_rectangle = name, (xmin, ymin), xmax - xmin, ymax - ymin\n",
    "        current_symbol = name, xmin, xmax, ymin, ymax\n",
    "#         rectangles.append(current_rectangle)\n",
    "        symbols.append(current_symbol)\n",
    "        symbol_names.add(name)\n",
    "    return symbols, symbol_names, int(height), int(width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the whole cell if you want to regenerate symbol set\n",
    "# class_dict = {}\n",
    "# symbol_type = set()\n",
    "# # form a universal symbol set fot the whole dataset, this can take 2 ~ 3 min\n",
    "# for x in xml_sorted:\n",
    "#     _, symbol_names,_ = get_symbol_info(x)\n",
    "#     symbol_type = symbol_type.union(symbol_names)\n",
    "\n",
    "# # save the symbol_type set for convenience# save t \n",
    "# np.save('symbol_type.npy', symbol_type) \n",
    "# # Load the dictionary\n",
    "# symbol_type = np.load('symbol_type.npy').item()\n",
    "# print('Total num of symbols in the dictionary: %d' % (len(symbol_type)))\n",
    "# i = 0\n",
    "# for item in symbol_type:\n",
    "#     class_dict[item] = i\n",
    "#     i += 1\n",
    "# print(class_dict['fClef'])\n",
    "# # save the class dictionary for futre use so that the integer class label does not change every time\n",
    "# np.save('class_dict.npy', class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of symbols in the whole dataset: 114\n",
      "The integer value for fClef is: 85\n"
     ]
    }
   ],
   "source": [
    "# uncomment this cell if you want to load previous symbol dict\n",
    "class_dict = np.load('class_dict.npy').item()\n",
    "print('Total number of symbols in the whole dataset:', len(class_dict))\n",
    "print('The integer value for fClef is:', class_dict['fClef'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "# the directory where deepscore folder is in\n",
    "dataset_dir = '../../'\n",
    "# The former split number of data used as training data\n",
    "# The latter num_samples - split number of data used as validation data\n",
    "split = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset is very large, can take 1~3 minutes\n",
    "# Training dataset\n",
    "dataset_train = ScoreDataset()\n",
    "dataset_train.load_score(dataset_dir, \"train\", split)\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ScoreDataset()\n",
    "dataset_val.load_score(dataset_dir, \"val\", split)\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Count in training set: 2\n",
      "Class Count: 115\n"
     ]
    }
   ],
   "source": [
    "print(\"Image Count in training set: {}\".format(len(dataset_train.image_ids)))\n",
    "print(\"Class Count: {}\".format(dataset_train.num_classes))\n",
    "# for i, info in enumerate(dataset_train.class_info):\n",
    "#     print(\"{:3}. {:50}\".format(i, info['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Count in validation set: 0\n",
      "Class Count: 115\n"
     ]
    }
   ],
   "source": [
    "print(\"Image Count in validation set: {}\".format(len(dataset_val.image_ids)))\n",
    "print(\"Class Count: {}\".format(dataset_val.num_classes))\n",
    "# for i, info in enumerate(dataset_val.class_info):\n",
    "#     print(\"{:3}. {:50}\".format(i, info['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCropConfig(ScoreConfig):\n",
    "    IMAGE_RESIZE_MODE = \"crop\"\n",
    "    IMAGE_MIN_DIM = 256\n",
    "    IMAGE_MAX_DIM = 256\n",
    "\n",
    "crop_config = RandomCropConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  16368\n",
      "Scales:  (8, 16, 32, 64, 128)\n",
      "ratios:  [0.5, 1, 2]\n",
      "Anchors per Cell:  3\n",
      "Levels:  5\n",
      "Anchors in Level 0: 12288\n",
      "Anchors in Level 1: 3072\n",
      "Anchors in Level 2: 768\n",
      "Anchors in Level 3: 192\n",
      "Anchors in Level 4: 48\n",
      "Level 0. Anchors:  12288  Feature map Shape: [64 64]\n",
      "Level 1. Anchors:   3072  Feature map Shape: [32 32]\n",
      "Level 2. Anchors:    768  Feature map Shape: [16 16]\n",
      "Level 3. Anchors:    192  Feature map Shape: [8 8]\n",
      "Level 4. Anchors:     48  Feature map Shape: [4 4]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28c002c9048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Visualize anchors of one cell at the center of the feature map\n",
    "\n",
    "# Load and display random image\n",
    "image_id = np.random.choice(dataset_train.image_ids, 1)[0]\n",
    "image, image_meta, _, _, _ = modellib.load_image_gt(dataset_train, crop_config, image_id)\n",
    "\n",
    "# Generate Anchors\n",
    "backbone_shapes = modellib.compute_backbone_shapes(config, image.shape)\n",
    "anchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES, \n",
    "                                          config.RPN_ANCHOR_RATIOS,\n",
    "                                          backbone_shapes,\n",
    "                                          config.BACKBONE_STRIDES, \n",
    "                                          config.RPN_ANCHOR_STRIDE)\n",
    "\n",
    "# Print summary of anchors\n",
    "num_levels = len(backbone_shapes)\n",
    "anchors_per_cell = len(config.RPN_ANCHOR_RATIOS)\n",
    "print(\"Count: \", anchors.shape[0])\n",
    "print(\"Scales: \", config.RPN_ANCHOR_SCALES)\n",
    "print(\"ratios: \", config.RPN_ANCHOR_RATIOS)\n",
    "print(\"Anchors per Cell: \", anchors_per_cell)\n",
    "print(\"Levels: \", num_levels)\n",
    "anchors_per_level = []\n",
    "for l in range(num_levels):\n",
    "    num_cells = backbone_shapes[l][0] * backbone_shapes[l][1]\n",
    "    anchors_per_level.append(anchors_per_cell * num_cells // config.RPN_ANCHOR_STRIDE**2)\n",
    "    print(\"Anchors in Level {}: {}\".format(l, anchors_per_level[l]))\n",
    "\n",
    "# Display\n",
    "fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "ax.imshow(image)\n",
    "levels = len(backbone_shapes)\n",
    "\n",
    "for level in range(levels):\n",
    "    colors = visualize.random_colors(levels)\n",
    "    # Compute the index of the anchors at the center of the image\n",
    "    level_start = sum(anchors_per_level[:level]) # sum of anchors of previous levels\n",
    "    level_anchors = anchors[level_start:level_start+anchors_per_level[level]]\n",
    "    print(\"Level {}. Anchors: {:6}  Feature map Shape: {}\".format(level, level_anchors.shape[0], \n",
    "                                                                backbone_shapes[level]))\n",
    "    center_cell = backbone_shapes[level] // 2\n",
    "    center_cell_index = (center_cell[0] * backbone_shapes[level][1] + center_cell[1])\n",
    "    level_center = center_cell_index * anchors_per_cell \n",
    "    center_anchor = anchors_per_cell * (\n",
    "        (center_cell[0] * backbone_shapes[level][1] / config.RPN_ANCHOR_STRIDE**2) \\\n",
    "        + center_cell[1] / config.RPN_ANCHOR_STRIDE)\n",
    "    level_center = int(center_anchor)\n",
    "\n",
    "    # Draw anchors. Brightness show the order in the array, dark to bright.\n",
    "    for i, rect in enumerate(level_anchors[level_center:level_center+anchors_per_cell]):\n",
    "        y1, x1, y2, x2 = rect\n",
    "        p = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, facecolor='none',\n",
    "                              edgecolor=(i+1)*np.array(colors[level]) / anchors_per_cell)\n",
    "        ax.add_patch(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generator\n",
    "random_rois = 50\n",
    "g = modellib.data_generator(\n",
    "    dataset_train, crop_config, shuffle=True, random_rois=random_rois, \n",
    "    batch_size=1,\n",
    "    detection_targets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-cba6e09c54d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get Next Image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrandom_rois\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;33m[\u001b[0m\u001b[0mnormalized_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrpn_match\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrpn_bbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_class_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_boxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_masks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrpn_rois\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrois\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[0mmrcnn_class_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmrcnn_bbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmrcnn_mask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rois\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrois\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mask_rcnn-2.1-py3.6.egg\\mrcnn\\model.py\u001b[0m in \u001b[0;36mdata_generator\u001b[1;34m(dataset, config, shuffle, augment, augmentation, random_rois, batch_size, detection_targets)\u001b[0m\n\u001b[0;32m   1693\u001b[0m                 load_image_gt(dataset, config, image_id, augment=augment,\n\u001b[0;32m   1694\u001b[0m                               \u001b[0maugmentation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maugmentation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m                               use_mini_mask=config.USE_MINI_MASK)\n\u001b[0m\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m             \u001b[1;31m# Skip images that have no instances. This can happen in cases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mask_rcnn-2.1-py3.6.egg\\mrcnn\\model.py\u001b[0m in \u001b[0;36mload_image_gt\u001b[1;34m(dataset, config, image_id, augment, augmentation, use_mini_mask)\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mmax_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMAGE_MAX_DIM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m         mode=config.IMAGE_RESIZE_MODE)\n\u001b[1;32m-> 1218\u001b[1;33m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1220\u001b[0m     \u001b[1;31m# Random horizontal flips.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mask_rcnn-2.1-py3.6.egg\\mrcnn\\utils.py\u001b[0m in \u001b[0;36mresize_mask\u001b[1;34m(mask, scale, padding, crop)\u001b[0m\n\u001b[0;32m    511\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 513\u001b[1;33m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzoom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzoom\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    514\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcrop\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\ndimage\\interpolation.py\u001b[0m in \u001b[0;36mzoom\u001b[1;34m(input, zoom, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[0;32m    626\u001b[0m                                                    shape=output_shape)\n\u001b[0;32m    627\u001b[0m     \u001b[0mzoom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzoom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m     \u001b[0m_nd_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzoom_shift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzoom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get Next Image\n",
    "if random_rois:\n",
    "    [normalized_images, image_meta, rpn_match, rpn_bbox, gt_class_ids, gt_boxes, gt_masks, rpn_rois, rois], \\\n",
    "    [mrcnn_class_ids, mrcnn_bbox, mrcnn_mask] = next(g)\n",
    "    \n",
    "    log(\"rois\", rois)\n",
    "    log(\"mrcnn_class_ids\", mrcnn_class_ids)\n",
    "    log(\"mrcnn_bbox\", mrcnn_bbox)\n",
    "    log(\"mrcnn_mask\", mrcnn_mask)\n",
    "else:\n",
    "    [normalized_images, image_meta, rpn_match, rpn_bbox, gt_boxes, gt_masks], _ = next(g)\n",
    "    \n",
    "log(\"gt_class_ids\", gt_class_ids)\n",
    "log(\"gt_boxes\", gt_boxes)\n",
    "log(\"gt_masks\", gt_masks)\n",
    "log(\"rpn_match\", rpn_match, )\n",
    "log(\"rpn_bbox\", rpn_bbox)\n",
    "image_id = modellib.parse_image_meta(image_meta)[\"image_id\"][0]\n",
    "print(\"image_id: \", image_id, dataset_train.image_reference(image_id))\n",
    "\n",
    "# Remove the last dim in mrcnn_class_ids. It's only added\n",
    "# to satisfy Keras restriction on target shape.\n",
    "mrcnn_class_ids = mrcnn_class_ids[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchors                  shape: (16368, 4)            min:  -90.50967  max:  282.50967  float64\n",
      "refined_anchors          shape: (3, 4)                min:  130.00000  max:  256.00000  float32\n",
      "Positive anchors: 3\n",
      "Negative anchors: 253\n",
      "Neutral anchors: 16112\n",
      "BG                     : 504\n",
      "accidentalNatural      : 5\n",
      "noteheadBlack          : 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28c004de518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = 0\n",
    "\n",
    "# Restore original image (reverse normalization)\n",
    "sample_image = modellib.unmold_image(normalized_images[b], config)\n",
    "\n",
    "# Compute anchor shifts.\n",
    "indices = np.where(rpn_match[b] == 1)[0]\n",
    "refined_anchors = utils.apply_box_deltas(anchors[indices], rpn_bbox[b, :len(indices)] * config.RPN_BBOX_STD_DEV)\n",
    "log(\"anchors\", anchors)\n",
    "log(\"refined_anchors\", refined_anchors)\n",
    "\n",
    "# Get list of positive anchors\n",
    "positive_anchor_ids = np.where(rpn_match[b] == 1)[0]\n",
    "print(\"Positive anchors: {}\".format(len(positive_anchor_ids)))\n",
    "negative_anchor_ids = np.where(rpn_match[b] == -1)[0]\n",
    "print(\"Negative anchors: {}\".format(len(negative_anchor_ids)))\n",
    "neutral_anchor_ids = np.where(rpn_match[b] == 0)[0]\n",
    "print(\"Neutral anchors: {}\".format(len(neutral_anchor_ids)))\n",
    "\n",
    "# ROI breakdown by class\n",
    "for c, n in zip(dataset_train.class_names, np.bincount(mrcnn_class_ids[b].flatten())):\n",
    "    if n:\n",
    "        print(\"{:23}: {}\".format(c[:20], n))\n",
    "\n",
    "# Show positive anchors\n",
    "fig, ax = plt.subplots(1, figsize=(16, 16))\n",
    "visualize.draw_boxes(sample_image, boxes=anchors[positive_anchor_ids], \n",
    "                     refined_boxes=refined_anchors, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights from pretrained model instead of from scratch\n",
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last()[1], by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /Users/Patty/Documents/stanford/courses/cs231n/project/fp/coop/Mask_RCNN/logs/symbols20180602T2251/mask_rcnn_symbols_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:2087: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-83fb3ae74319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             layers='heads')\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation)\u001b[0m\n\u001b[1;32m   2326\u001b[0m             \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2328\u001b[0;31m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2329\u001b[0m         )\n\u001b[1;32m   2330\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2192\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2193\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2194\u001b[0;31m                     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2196\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    785\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0;31m# Make sure to rethrow the first exception in the queue, if any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()[1]\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
